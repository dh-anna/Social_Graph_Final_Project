{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "x798dcolcqi",
   "source": [
    "## Download datasets to analyze\n",
    "This dataset requires kagglehub. Install it with:\n",
    "```bash\n",
    "pip install kagglehub[pandas-datasets]\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "dzsl42s0r1t",
   "source": [
    "import shutil\n",
    "import gzip\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# Define paths using pathlib.Path\n",
    "datasets_dir = Path(\"Datasets\")\n",
    "imdb_dir = Path(\"Datasets/IMDB\")\n",
    "\n",
    "# Headers to mimic a browser request\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'Accept': '*/*',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "}\n",
    "\n",
    "\n",
    "def download_file_with_progress(url: str, dest_path: Path):\n",
    "    \"\"\"Download a file with progress indication, handling large files properly.\"\"\"\n",
    "    print(f\"  Downloading from: {url}\")\n",
    "    \n",
    "    # Use a session for connection pooling\n",
    "    session = requests.Session()\n",
    "    session.headers.update(HEADERS)\n",
    "    \n",
    "    try:\n",
    "        response = session.get(url, stream=True, timeout=(10, 300))  # 10s connect, 300s read timeout\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        print(f\"  File size: {total_size / (1024*1024):.1f} MB (compressed)\")\n",
    "        downloaded = 0\n",
    "        \n",
    "        with open(dest_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024 * 1024):  # 1MB chunks\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    downloaded += len(chunk)\n",
    "                    if total_size > 0:\n",
    "                        progress = (downloaded / total_size) * 100\n",
    "                        print(f\"\\r  Progress: {progress:.1f}% ({downloaded / (1024*1024):.1f} MB)\", end=\"\", flush=True)\n",
    "        \n",
    "        print()  # New line after progress\n",
    "        return True\n",
    "        \n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"\\n  ERROR: Download timed out\")\n",
    "        raise\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"\\n  ERROR: HTTP error {e.response.status_code}: {e.response.reason}\")\n",
    "        raise\n",
    "    except requests.exceptions.ConnectionError as e:\n",
    "        print(f\"\\n  ERROR: Connection failed: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def download_from_kaggle(kaggle_dataset: str, target_filename: str, dest_path: Path):\n",
    "    \"\"\"Download a dataset from Kaggle and copy the target file to dest_path.\"\"\"\n",
    "    import kagglehub\n",
    "    \n",
    "    print(f\"  Downloading dataset: {kaggle_dataset}\")\n",
    "    \n",
    "    # Download the dataset (returns path to cached directory)\n",
    "    cache_path = Path(kagglehub.dataset_download(kaggle_dataset))\n",
    "    print(f\"  Downloaded to cache: {cache_path}\")\n",
    "    \n",
    "    # Find the target file in the downloaded dataset\n",
    "    # Try exact match first, then case-insensitive search\n",
    "    found_file = None\n",
    "    \n",
    "    # List all files in the cache\n",
    "    all_files = list(cache_path.rglob(\"*\"))\n",
    "    csv_files = [f for f in all_files if f.is_file() and f.suffix.lower() == '.csv']\n",
    "    \n",
    "    print(f\"  Found {len(csv_files)} CSV file(s) in dataset\")\n",
    "    \n",
    "    # Try to find exact match\n",
    "    for f in csv_files:\n",
    "        if f.name == target_filename:\n",
    "            found_file = f\n",
    "            break\n",
    "    \n",
    "    # If no exact match, try case-insensitive\n",
    "    if not found_file:\n",
    "        for f in csv_files:\n",
    "            if f.name.lower() == target_filename.lower():\n",
    "                found_file = f\n",
    "                break\n",
    "    \n",
    "    # If still no match and there's only one CSV, use that\n",
    "    if not found_file and len(csv_files) == 1:\n",
    "        found_file = csv_files[0]\n",
    "        print(f\"  Using only CSV file found: {found_file.name}\")\n",
    "    \n",
    "    if not found_file:\n",
    "        available = [f.name for f in csv_files[:5]]  # Show first 5\n",
    "        raise FileNotFoundError(\n",
    "            f\"Could not find '{target_filename}' in dataset. \"\n",
    "            f\"Available CSV files: {available}\"\n",
    "        )\n",
    "    \n",
    "    # Copy the file to destination\n",
    "    print(f\"  Copying {found_file.name} -> {dest_path}\")\n",
    "    shutil.copy2(found_file, dest_path)\n",
    "    return True\n",
    "\n",
    "\n",
    "def download_all_datasets():\n",
    "    print(\"Dataset downloading started.\")\n",
    "\n",
    "    # Check and create Datasets folder if it doesn't exist\n",
    "    if not datasets_dir.exists():\n",
    "        print(f\"Creating Datasets folder: {datasets_dir}\")\n",
    "        datasets_dir.mkdir(parents=True, exist_ok=True)\n",
    "    else:\n",
    "        print(f\"Datasets folder exists: {datasets_dir}\")\n",
    "\n",
    "    # Check and create IMDB subfolder if it doesn't exist\n",
    "    if not imdb_dir.exists():\n",
    "        print(f\"Creating IMDB folder: {imdb_dir}\")\n",
    "        imdb_dir.mkdir(parents=True, exist_ok=True)\n",
    "    else:\n",
    "        print(f\"IMDB folder exists: {imdb_dir}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    total_downloaded = 0\n",
    "    total_skipped = 0\n",
    "    total_failed = 0\n",
    "    \n",
    "    # Define all datasets with their download methods\n",
    "    datasets_to_check = [\n",
    "        {\n",
    "            \"name\": \"actorfilms.csv\",\n",
    "            \"path\": imdb_dir / \"actorfilms.csv\",\n",
    "            \"source\": \"kaggle\",\n",
    "            \"kaggle_dataset\": \"darinhawley/imdb-films-by-actor-for-10k-actors\",\n",
    "            \"kaggle_filename\": \"actorfilms.csv\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Celebrity.csv\",\n",
    "            \"path\": datasets_dir / \"Celebrity.csv\",\n",
    "            \"source\": \"kaggle\",\n",
    "            \"kaggle_dataset\": \"madhuripanchakshri/top-10000-celebrities-dataset\",\n",
    "            \"kaggle_filename\": \"Celebrity.csv\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"IMDb movies.csv\",\n",
    "            \"path\": datasets_dir / \"IMDb movies.csv\",\n",
    "            \"source\": \"kaggle\",\n",
    "            \"kaggle_dataset\": \"simhyunsu/imdbextensivedataset\",\n",
    "            \"kaggle_filename\": \"IMDb movies.csv\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"title.crew.tsv\",\n",
    "            \"path\": imdb_dir / \"title.crew.tsv\",\n",
    "            \"source\": \"url\",\n",
    "            \"url\": \"a/title.crew.tsv.gz\",\n",
    "            \"decompress\": True,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"name.basics.tsv\",\n",
    "            \"path\": imdb_dir / \"name.basics.tsv\",\n",
    "            \"source\": \"url\",\n",
    "            \"url\": \"https://datasets.imdbws.com/name.basics.tsv.gz\",\n",
    "            \"decompress\": True,\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    # Check and download each dataset\n",
    "    for dataset_info in datasets_to_check:\n",
    "        name = dataset_info[\"name\"]\n",
    "        path = dataset_info[\"path\"]\n",
    "\n",
    "        print(f\"Checking: {name}\")\n",
    "        \n",
    "        # Check if dataset already exists\n",
    "        if path.exists():\n",
    "            file_size = path.stat().st_size / (1024 * 1024)\n",
    "            print(f\"Already exists ({file_size:.2f} MB)\")\n",
    "            print(f\"Skipping download\")\n",
    "            total_skipped += 1\n",
    "            continue\n",
    "        \n",
    "        # Dataset is missing, attempt download\n",
    "        print(f\"Missing - attempting download...\")\n",
    "        \n",
    "        try:\n",
    "            # Create parent directory if needed\n",
    "            path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            if dataset_info[\"source\"] == \"kaggle\":\n",
    "                # Download from Kaggle using kagglehub\n",
    "                try:\n",
    "                    print(f\"  Source: Kaggle ({dataset_info['kaggle_dataset']})\")\n",
    "                    \n",
    "                    download_from_kaggle(\n",
    "                        dataset_info['kaggle_dataset'],\n",
    "                        dataset_info['kaggle_filename'],\n",
    "                        path\n",
    "                    )\n",
    "                    \n",
    "                    file_size = path.stat().st_size / (1024 * 1024)\n",
    "                    print(f\"Successfully downloaded ({file_size:.2f} MB)\")\n",
    "                    total_downloaded += 1\n",
    "                    \n",
    "                except ImportError:\n",
    "                    print(f\"Failed: kagglehub not installed\")\n",
    "                    print(f\"Install with: pip install kagglehub\")\n",
    "                    total_failed += 1\n",
    "                    \n",
    "            elif dataset_info[\"source\"] == \"url\":\n",
    "                # Download from URL\n",
    "                url = dataset_info[\"url\"]\n",
    "                decompress = dataset_info.get(\"decompress\", False)\n",
    "                \n",
    "                if decompress and url.endswith('.gz'):\n",
    "                    # Download compressed file first\n",
    "                    temp_gz = path.with_suffix(path.suffix + '.gz')\n",
    "                    \n",
    "                    download_file_with_progress(url, temp_gz)\n",
    "                    \n",
    "                    # Verify the downloaded file exists and has content\n",
    "                    if not temp_gz.exists() or temp_gz.stat().st_size == 0:\n",
    "                        raise Exception(f\"Downloaded file is empty or missing: {temp_gz}\")\n",
    "                    \n",
    "                    # Decompress the file\n",
    "                    print(f\"  Extracting {temp_gz.name} -> {path.name}...\")\n",
    "                    with gzip.open(temp_gz, 'rb') as f_in:\n",
    "                        with open(path, 'wb') as f_out:\n",
    "                            shutil.copyfileobj(f_in, f_out)\n",
    "                    \n",
    "                    # Remove temporary compressed file\n",
    "                    temp_gz.unlink()\n",
    "                    print(f\"  Removed temporary file: {temp_gz.name}\")\n",
    "                else:\n",
    "                    # Direct download\n",
    "                    download_file_with_progress(url, path)\n",
    "                \n",
    "                file_size = path.stat().st_size / (1024 * 1024)\n",
    "                print(f\"Successfully downloaded ({file_size:.2f} MB)\")\n",
    "                total_downloaded += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Failed: {type(e).__name__}: {e}\")\n",
    "            # Clean up any partial downloads\n",
    "            if path.exists():\n",
    "                path.unlink()\n",
    "            temp_gz = path.with_suffix(path.suffix + '.gz')\n",
    "            if temp_gz.exists():\n",
    "                temp_gz.unlink()\n",
    "            total_failed += 1\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n\\nTotal datasets checked: {len(datasets_to_check)}\")\n",
    "    print(f\"Already present:        {total_skipped}\")\n",
    "    print(f\"Downloaded:             {total_downloaded}\")\n",
    "    print(f\"Failed:                 {total_failed}\")\n",
    "    \n",
    "    if total_failed == 0 and total_skipped + total_downloaded == len(datasets_to_check):\n",
    "        print(f\"\\nAll {len(datasets_to_check)} datasets are now available!\")\n",
    "    elif total_failed > 0:\n",
    "        print(f\"\\n Warning: {total_failed} dataset(s) failed to download\")\n",
    "\n",
    "    \n",
    "    return {\n",
    "        \"total\": len(datasets_to_check),\n",
    "        \"skipped\": total_skipped,\n",
    "        \"downloaded\": total_downloaded,\n",
    "        \"failed\": total_failed\n",
    "    }\n",
    "\n",
    "# Run the pipeline\n",
    "result = download_all_datasets()"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T19:50:02.645902Z",
     "start_time": "2025-11-25T19:49:03.161687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloading started.\n",
      "Creating Datasets folder: Datasets\n",
      "Creating IMDB folder: Datasets/IMDB\n",
      "\n",
      "Checking: actorfilms.csv\n",
      "Missing - attempting download...\n",
      "  Source: Kaggle (darinhawley/imdb-films-by-actor-for-10k-actors)\n",
      "  Downloading dataset: darinhawley/imdb-films-by-actor-for-10k-actors\n",
      "  Downloaded to cache: /Users/kostyalbalint/.cache/kagglehub/datasets/darinhawley/imdb-films-by-actor-for-10k-actors/versions/1\n",
      "  Found 1 CSV file(s) in dataset\n",
      "  Copying actorfilms.csv -> Datasets/IMDB/actorfilms.csv\n",
      "Successfully downloaded (11.96 MB)\n",
      "Checking: Celebrity.csv\n",
      "Missing - attempting download...\n",
      "  Source: Kaggle (madhuripanchakshri/top-10000-celebrities-dataset)\n",
      "  Downloading dataset: madhuripanchakshri/top-10000-celebrities-dataset\n",
      "  Downloaded to cache: /Users/kostyalbalint/.cache/kagglehub/datasets/madhuripanchakshri/top-10000-celebrities-dataset/versions/1\n",
      "  Found 1 CSV file(s) in dataset\n",
      "  Copying Celebrity.csv -> Datasets/Celebrity.csv\n",
      "Successfully downloaded (0.59 MB)\n",
      "Checking: IMDb movies.csv\n",
      "Missing - attempting download...\n",
      "  Source: Kaggle (simhyunsu/imdbextensivedataset)\n",
      "  Downloading dataset: simhyunsu/imdbextensivedataset\n",
      "  Downloaded to cache: /Users/kostyalbalint/.cache/kagglehub/datasets/simhyunsu/imdbextensivedataset/versions/1\n",
      "  Found 2 CSV file(s) in dataset\n",
      "  Copying IMDb movies.csv -> Datasets/IMDb movies.csv\n",
      "Successfully downloaded (45.62 MB)\n",
      "Checking: title.crew.tsv\n",
      "Missing - attempting download...\n",
      "  Downloading from: https://datasets.imdbws.com/title.crew.tsv.gz\n",
      "  File size: 75.2 MB (compressed)\n",
      "  Progress: 100.0% (75.2 MB)\n",
      "  Extracting title.crew.tsv.gz -> title.crew.tsv...\n",
      "  Removed temporary file: title.crew.tsv.gz\n",
      "Successfully downloaded (380.04 MB)\n",
      "Checking: name.basics.tsv\n",
      "Missing - attempting download...\n",
      "  Downloading from: https://datasets.imdbws.com/name.basics.tsv.gz\n",
      "  File size: 281.2 MB (compressed)\n",
      "  Progress: 100.0% (281.2 MB)\n",
      "  Extracting name.basics.tsv.gz -> name.basics.tsv...\n",
      "  Removed temporary file: name.basics.tsv.gz\n",
      "Successfully downloaded (878.38 MB)\n",
      "\n",
      "\n",
      "Total datasets checked: 5\n",
      "Already present:        0\n",
      "Downloaded:             5\n",
      "Failed:                 0\n",
      "\n",
      "All 5 datasets are now available!\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
