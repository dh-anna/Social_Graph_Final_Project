{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "x798dcolcqi",
   "source": [
    "## Download datasets to analyze\n",
    "This dataset requires kagglehub. Install it with:\n",
    "```bash\n",
    "pip install kagglehub[pandas-datasets]\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "dzsl42s0r1t",
   "source": [
    "import shutil\n",
    "import gzip\n",
    "import urllib\n",
    "\n",
    "\n",
    "def download_all_datasets():\n",
    "    print(\"Dataset downloading started.\")\n",
    "\n",
    "    if not datasets_dir.exists():\n",
    "        print(f\"Creating Datasets folder: {datasets_dir}\")\n",
    "        datasets_dir.mkdir(parents=True, exist_ok=True)\n",
    "    else:\n",
    "        print(f\"Datasets folder exists: {datasets_dir}\")\n",
    "\n",
    "    if not imdb_dir.exists():\n",
    "        print(f\"Creating IMDB folder: {imdb_dir}\")\n",
    "        imdb_dir.mkdir(parents=True, exist_ok=True)\n",
    "    else:\n",
    "        print(f\"IMDB folder exists: {imdb_dir}\")\n",
    "\n",
    "\n",
    "    total_downloaded = 0\n",
    "    total_skipped = 0\n",
    "    total_failed = 0\n",
    "    \n",
    "    # Define all datasets with their download methods\n",
    "    datasets_to_check = [\n",
    "        {\n",
    "            \"name\": \"actorfilms.csv\",\n",
    "            \"path\": imdb_dir / \"actorfilms.csv\",\n",
    "            \"source\": \"kaggle\",\n",
    "            \"kaggle_dataset\": \"darinhawley/imdb-films-by-actor-for-10k-actors\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Celebrity.csv\",\n",
    "            \"path\": datasets_dir / \"Celebrity.csv\",\n",
    "            \"source\": \"kaggle\",\n",
    "            \"kaggle_dataset\": \"madhuripanchakshri/top-10000-celebrities-dataset\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"IMDb movies.csv\",\n",
    "            \"path\": datasets_dir / \"IMDb movies.csv\",\n",
    "            \"source\": \"kaggle\",\n",
    "            \"kaggle_dataset\": \"simhyunsu/imdbextensivedataset\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"title.crew.tsv\",\n",
    "            \"path\": imdb_dir / \"title.crew.tsv\",\n",
    "            \"source\": \"url\",\n",
    "            \"url\": \"https://datasets.imdbws.com/title.crew.tsv.gz\",\n",
    "            \"decompress\": True,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"name.basics.tsv\",\n",
    "            \"path\": imdb_dir / \"name.basics.tsv\",\n",
    "            \"source\": \"url\",\n",
    "            \"url\": \"https://datasets.imdbws.com/name.basics.tsv.gz\",\n",
    "            \"decompress\": True,\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    # Check and download each dataset\n",
    "    for dataset_info in datasets_to_check:\n",
    "        name = dataset_info[\"name\"]\n",
    "        path = dataset_info[\"path\"]\n",
    "\n",
    "        print(f\"Checking: {name}\")\n",
    "        \n",
    "        # Check if dataset already exists\n",
    "        if path.exists():\n",
    "            file_size = path.stat().st_size / (1024 * 1024)\n",
    "            print(f\"Already exists ({file_size:.2f} MB)\")\n",
    "            print(f\"Skipping download\")\n",
    "            total_skipped += 1\n",
    "            continue\n",
    "        \n",
    "        # Dataset is missing, attempt download\n",
    "        print(f\"Missing - attempting download...\")\n",
    "        \n",
    "        try:\n",
    "            # Create parent directory if needed\n",
    "            path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            if dataset_info[\"source\"] == \"kaggle\":\n",
    "                # Download from Kaggle using kagglehub\n",
    "                try:\n",
    "                    import kagglehub\n",
    "                    from kagglehub import KaggleDatasetAdapter\n",
    "                    \n",
    "                    print(f\" Source: Kaggle ({dataset_info['kaggle_dataset']})\")\n",
    "                    \n",
    "                    df = kagglehub.load_dataset(\n",
    "                        KaggleDatasetAdapter.PANDAS,\n",
    "                        dataset_info['kaggle_dataset'],\n",
    "                        \"\",\n",
    "                    )\n",
    "                    \n",
    "                    # Save to CSV\n",
    "                    df.to_csv(path, index=True)\n",
    "                    \n",
    "                    file_size = path.stat().st_size / (1024 * 1024)\n",
    "                    print(f\"Successfully downloaded ({file_size:.2f} MB)\")\n",
    "                    total_downloaded += 1\n",
    "                    \n",
    "                except ImportError:\n",
    "                    print(f\"Failed: kagglehub not installed\")\n",
    "                    print(f\"Install with: pip install kagglehub[pandas-datasets]\")\n",
    "                    total_failed += 1\n",
    "                    \n",
    "            elif dataset_info[\"source\"] == \"url\":\n",
    "                # Download from URL\n",
    "                url = dataset_info[\"url\"]\n",
    "                decompress = dataset_info.get(\"decompress\", False)\n",
    "                \n",
    "                print(f\"  Source: {url}\")\n",
    "                \n",
    "                if decompress and url.endswith('.gz'):\n",
    "                    # Download and decompress\n",
    "                    temp_gz = path.with_suffix(path.suffix + '.gz')\n",
    "\n",
    "                    urllib.request.urlretrieve(url, temp_gz)\n",
    "\n",
    "                    with gzip.open(temp_gz, 'rb') as f_in:\n",
    "                        with open(path, 'wb') as f_out:\n",
    "                            shutil.copyfileobj(f_in, f_out)\n",
    "                    \n",
    "                    temp_gz.unlink()\n",
    "                else:\n",
    "                    # Direct download\n",
    "                    urllib.request.urlretrieve(url, path)\n",
    "                \n",
    "                file_size = path.stat().st_size / (1024 * 1024)\n",
    "                print(f\"Successfully downloaded ({file_size:.2f} MB)\")\n",
    "                total_downloaded += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Failed: {e}\")\n",
    "            total_failed += 1\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n\\nTotal datasets checked: {len(datasets_to_check)}\")\n",
    "    print(f\"Already present:        {total_skipped}\")\n",
    "    print(f\"Downloaded:             {total_downloaded}\")\n",
    "    print(f\"Failed:                 {total_failed}\")\n",
    "    \n",
    "    if total_failed == 0 and total_skipped + total_downloaded == len(datasets_to_check):\n",
    "        print(f\"\\nAll {len(datasets_to_check)} datasets are now available!\")\n",
    "    elif total_failed > 0:\n",
    "        print(f\"\\n Warning: {total_failed} dataset(s) failed to download\")\n",
    "\n",
    "    \n",
    "    return {\n",
    "        \"total\": len(datasets_to_check),\n",
    "        \"skipped\": total_skipped,\n",
    "        \"downloaded\": total_downloaded,\n",
    "        \"failed\": total_failed\n",
    "    }\n",
    "\n",
    "# Run the pipeline\n",
    "result = download_all_datasets()"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T19:22:32.893056Z",
     "start_time": "2025-11-25T19:22:32.881677Z"
    }
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
